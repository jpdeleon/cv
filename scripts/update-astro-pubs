#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
If run locally, get NASA ADS token from 
https://ui.adsabs.harvard.edu/user/settings/token
save it to a file called ~/.ads/dev_key

If run on GitHub Actions, set the token as a secret called ADS_DEV_KEY
"""
import os
import json
import importlib.util
from operator import itemgetter

import ads
# ads.config.token = 'my token'

# Get the current directory of the script
here = os.path.dirname(os.path.abspath(__file__))
# Import the 'utf8totex' module
spec = importlib.util.spec_from_file_location(
    "utf8totex", os.path.join(here, "utf8totex.py")
)
utf8totex = importlib.util.module_from_spec(spec)
spec.loader.exec_module(utf8totex)

authors = ["de Leon, J. P.", "Leon, J. P."]
# exclude papers with asteroid in abstract
search_query = 'aff:"University of Tokyo" database:astronomy -abs:asteroid'
# search_query += "year:2014-2025"

def get_papers(author):
    # Perform an ADS search query to retrieve information about the author's papers
    papers = list(
        ads.SearchQuery(
            q=search_query,
            author=author,
            fl=[
                "id",
                "title",
                "author",
                "doi",
                "year",
                "pubdate",
                "pub",
                "volume",
                "page",
                "identifier",
                "doctype",
                "citation_count",
                "bibcode",
            ],
            max_pages=100,
        )
    )
    dicts = []
    for paper in papers:
        # Process and filter arxiv id
        if paper.identifier is None:
            continue
        aid = [
            ":".join(t.split(":")[1:])
            for t in paper.identifier
            if t.startswith("arXiv:")
        ]
        for t in paper.identifier:
            if len(t.split(".")) != 2:
                continue
            try:
                list(map(int, t.split(".")))
            except ValueError:
                pass
            else:
                aid.append(t)
        try:
            page = int(paper.page[0])
        except (ValueError, TypeError):
            page = None
            if paper.page is not None and paper.page[0].startswith("arXiv:"):
                aid.append(":".join(paper.page[0].split(":")[1:]))
        try:
            formatted_authors = list(map(utf8totex.utf8totex, paper.author))
            formatted_titles = utf8totex.utf8totex(paper.title[0].replace(" &amp; ", " & "))
        except Exception as e:
            print(e)
            formatted_authors = list(paper.author)
            formatted_titles = paper.title[0].replace(" &amp; ", " & ")
        # Create a dictionary with relevant paper information
        dicts.append(
            dict(
                doctype=paper.doctype,
                authors=formatted_authors,
                year=paper.year,
                pubdate=paper.pubdate,
                doi=paper.doi[0] if paper.doi is not None else None,
                title=formatted_titles,
                pub=paper.pub,
                volume=paper.volume,
                page=page,
                arxiv=aid[0] if len(aid) else None,
                citations=(
                    paper.citation_count if paper.citation_count is not None else 0
                ),
                url="https://ui.adsabs.harvard.edu/abs/" + paper.bibcode,
            )
        )
    return sorted(dicts, key=itemgetter("pubdate"), reverse=True)


if __name__ == "__main__":
    papers = []
    for author in authors:
        paper = get_papers(author)
        papers.extend(paper)
    papers = sorted(papers, key=itemgetter("pubdate"), reverse=True)
    
    fout = "data/pubs.json"
    with open(fout, "w") as f:
        json.dump(papers, f, sort_keys=True, indent=2, separators=(",", ": "))
    print("Saved: ", fout)